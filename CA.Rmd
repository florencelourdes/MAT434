---
title: "Competition Assignment"
author: "Florence Lourdes"
date: "2023-09-27"
output: html_document
statement of purpose: "I am training a model to predict the price range of houses depending on some variables. I am doing it for people who are looking to buy a house or sell their house. They can benefit by getting information about what they should be expecting when it comes to certain price range houses."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("tidyverse")
#install.packages("corrr")
#install.packages("ranger")
#install.packages("yardstick")
#install.packages("tidytext")
library("yardstick")
library(corrr)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(patchwork)
library(ggplot2)
library(ranger)
library(tidytext)
#install.packages('xgboost')
#require(xgboost)
#data(agaricus.train, package='xgboost')
#data(agaricus.test, package='xgboost')
#train <- agaricus.train
#test <- agaricus.test
data <- read_csv("C:/MAT434/data.csv")
comp <- read_csv("C:/MAT434/comp.csv")
```
# Head of Data
The following table shows the top 6 rows of the data
```{r}
data %>%
  head()
```

# Set a seed for reproducibility
Setting a seed make sure that we get the same results for randomization
```{r}
set.seed(123)
```

# Split data into training and test sets
Splitting data into a training and test sets with a 8 to 2 ratio and marking price range as prediction target 
```{r}
split_data <- initial_split(data, prop = 0.8, strata = priceRange)  # Adjust ratio and strata as needed
```

# Extract the training and test sets
Extracting the data from each training and test sets. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis
```{r}
train_data <- training(split_data)
test_data <- testing(split_data)
```

# Build cross-validation folds
K-fold cross-validation is a technique for evaluating predictive models. The dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance.
```{r}
set.seed(456)
train_folds <- vfold_cv(train_data, v = 10)
```

# Exploratory Data Analysis (EDA)
EDA is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.
```{r}
# Summary statistics
summary(train_data)

# Bar plots for categorical variables
ggplot(train_data, aes(x = priceRange, y=numOfBedrooms)) + 
  geom_bar(stat = "identity")

# Scatterplots for relationships
ggplot(train_data, aes(x=priceRange, y=numOfBathrooms)) + 
    geom_point()

# Correlation matrix
data_cor <- train_data %>% 
  select(numOfBedrooms, lotSizeSqFt, priceRange) %>% 
  correlate()
```

# Training a Model

```{r}
# Create a model specification
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Create a recipe
log_reg_rec <- recipe(priceRange ~ ., data = train_data) %>%
  step_rm(id, description, latitude, longitude, avgSchoolRating, MedianStudentsPerTeacher) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) #add recipe steps as needed

# Create a workflow
log_reg_wf <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(log_reg_rec)

# Run cross-validation to obtain cross-validation performance estimate
log_reg_cv_results <- log_reg_wf %>%
  fit_resamples(train_folds)

# Collect cross-validation results
#log_reg_cv_results %>%
  #collect_metrics()

# Fit model to training data
#log_reg_fit <- log_reg_wf %>%
  #fit(train_data)

# Assess model on test data
#log_reg_wf %>%
  #augment(test) %>%
  #my_metrics(response, .pred)

# Use model to predict for new data
#log_reg_fit %>%
  #augment(new_data)
```

# Model Construction and Interpretation
In the following code, we will be using two training models: nearest neighbor and decision tree.The steps are to make a workflow, evaluate, and print the result.

```{r}
# KNN workflow
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

# Decision tree workflow
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

# Evaluate workflows
knn_cv_results <- knn_wf %>%
  fit_resamples(train_folds)

dt_cv_results <- dt_wf %>%
  fit_resamples(train_folds)
```

```{r}
# KNN result
knn_cv_results %>%
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

```{r}
# DT result
dt_cv_results %>%
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


Result Summary: the decision tree model has higher accuracy values

Next, we will be using hyperparameter tuning. Hyperparameter tuning allows data scientists to tweak model performance for optimal results. This process is an essential part of machine learning, and choosing appropriate hyperparameter values is crucial for success.

We will start off by setting the grid parameter of tune_grid() to a number which will create a “space filling” grid containing that number of hyperparameter combinations. These are essentially tibbles (data frames) of options for each hyperparameter.

```{r}

# Grid
grid_neighbors <- tibble(
  "neighbors" = c(1, 3, 5, 7, 11, 15, 21, 41)
  )

grid_depth <- tibble(
  "tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20)
  )
```

The next step is to re-create the model workflows, setting the relevant hyperparameters to tune(). Then we’ll pipe these workflows into tune_grid() and read the results.

```{r}
# KNN workflow
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

# DT workflow
dt_spec <- decision_tree(tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

# Hyperparameter tune
knn_tune_results <- knn_wf %>%
  tune_grid(
    grid = grid_neighbors,
    resamples = train_folds
  )

dt_tune_results <- dt_wf %>%
  tune_grid(
    grid = grid_depth,
    resamples = train_folds
  )

```

Here is the nearest neighbor result:
```{r}
# KNN result 
knn_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(neighbors, mean, std_err) %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Here is the decission tree result:
```{r}
# DT results
dt_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(tree_depth, mean, std_err) %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


Result: The optimized nearest neighbor classifier has higher accuracy than the decision tree classifier. The accuracy for this model is about 38% which is slightly higher than the initial nearest neighbor result of 32%. It also beats decision tree model which has 37% accuracy. This proves that hyperparameter tuning can improve model and increase accuracy.

After deciding on a model with the highest accuracy result, we will be fitting the model to do predictions on the competition data set. The predictions will then be exported as a csv file for submission.

```{r}
# Setting the grid with the highest accuracy
knn_spec <- nearest_neighbor(neighbors = 41) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

# Fitting workflow
knn_fit <- knn_wf %>%
  fit(data)

# Enter competition data
knn_fit %>%
  augment(comp)

# Select and rename columns
submission_df <- knn_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

# Export
write.csv(submission_df, "ZillowSubmission.csv", row.names = FALSE)
```


The next step is to explore another model to hopefully get a higher accuracy. The model that we are going to use now is random forest. A random forest is a form of bootstrap aggregation where we construct a decision tree model on each of the bootstrapped training sets. Rather than just constructing a decision tree though, we allow each tree access to only a random subset of predictors each time it makes a split. Since the trees are provided random access to the predictors, our trees won't all look alike. This means that the decision trees in our random forest ensemble won't make the same errors and the ensemble can benefit from the wisdom of the crowd.


```{r eval = FALSE, echo = FALSE, include = FALSE}
# Define random forest
rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

#Set recipe
rf_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal_predictors())

# Create workflow
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec)

# Fit model
rf_fit <- rf_wf %>%
  fit(data)

rf_fit %>%
  extract_fit_engine() %>%
  ranger::treeInfo(tree = 1)

# Evaluate workflows
rf_cv_results <- rf_wf %>%
  fit_resamples(train_folds)
```

```{r}
# Random Forest result
rf_cv_results %>%
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```
We get 40.3% as the accuracy result. It is a slight improvement of the tuned KNN model that had 38% accuracy. The next step is to do hyperparameter tuning like we did with the Nearest Neighbor and Decision Tree models to hopefully get a model with better accuracy. We are going to use Bootstrap. Bootstrapping is a widely used technique to generate new, hypothetical random samples. We treat our available sample data as if it were the population, and repeatedly draw random samples from it. These random samples have the same size as the original set of sample data and are drawn with replacement.

```{r}
# Specify the random forest model with hyperparameter grid
rf_spec <- rand_forest(trees = tune(), mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Set recipe
rf_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal_predictors())

# Create the workflow with the model specification and recipe
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec)

# Perform hyperparameter tuning
rf_tune_results <- rf_wf %>%
  tune_grid(
    resamples = rsample::bootstraps(train_data, times = 5),
    grid = expand.grid(trees = c(50, 100, 150, 200), mtry = c(2, 4, 6, 8))
  )

# Print the results of hyperparameter tuning
collect_metrics(rf_tune_results)
```
Unfortunately, none of the grids produced an accuracy higher than the untuned random forest. The hyperparameter grid used in the tuning process may not include values that improve the model's performance. It's essential to carefully choose the range and values for hyperparameters. Expanding the search space or consider different values for mtry and trees may give a higher accuracy value. For now, let's fit the untuned random forest model. 

```{r}
# Extract the best model
rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal_predictors())

# Create workflow
rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec)

# Fit model
rf_fit <- rf_wf %>%
  fit(data)

# Enter competition data
rf_fit %>%
  augment(comp)

# Select and rename columns
submission_df <- rf_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

# Export
write.csv(submission_df, "ZillowSubmission.csv", row.names = FALSE)
```

