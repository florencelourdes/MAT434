---
title: "Competition Assignment"
author: "Florence Lourdes"
date: "2023-09-27"
output: html_document
statement of purpose: "I am training a model to predict the price range of houses depending on some variables. 
I am doing it for people who are looking to buy a house or sell their house. They can benefit by getting information about what they should be expecting when it comes to certain price range houses."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("tidyverse")
#install.packages("corrr")
library(corrr)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(patchwork)
library(ggplot2)
#install.packages('xgboost')
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
data <- read_csv("C:/MAT434/data.csv")
```
# Head of Data
The following table shows the top 6 rows of the data
```{r}
data %>%
  head()
```

# Set a seed for reproducibility
Setting a seed make sure that we get the same results for randomization
```{r}
set.seed(123)
```

# Split data into training and test sets
Splitting data into a training and test sets with a 8 to 2 ratio and marking price range as prediction target 
```{r}
split_data <- initial_split(data, prop = 0.8, strata = priceRange)  # Adjust ratio and strata as needed
```

# Extract the training and test sets
Extracting the data from each training and test sets. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis
```{r}
train_data <- training(split_data)
test_data <- testing(split_data)
```

# Build cross-validation folds
K-fold cross-validation is a technique for evaluating predictive models. The dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance.
```{r}
set.seed(456)
train_folds <- vfold_cv(train_data, v = 10)
```

# Exploratory Data Analysis (EDA)
EDA is an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected. EDA is an important first step in any data analysis.
```{r}
# Summary statistics
summary(train_data)

# Bar plots for categorical variables
ggplot(train_data, aes(x = priceRange, y=numOfBedrooms)) + 
  geom_bar(stat = "identity")

# Scatterplots for relationships
ggplot(train_data, aes(x=priceRange, y=numOfBathrooms)) + 
    geom_point()

# Correlation matrix
data_cor <- train_data %>% 
  select(numOfBedrooms, lotSizeSqFt, priceRange) %>% 
  correlate()
```

# Training a Model

```{r}
#Create a model specification
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

#Create a recipe
log_reg_rec <- recipe(priceRange ~ ., data = train_data) %>%
  step_rm(id, description, latitude, longitude, avgSchoolRating, MedianStudentsPerTeacher) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) #add recipe steps as needed

#Create a workflow
log_reg_wf <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(log_reg_rec)

#Run cross-validation to obtain cross-validation performance estimate
log_reg_cv_results <- log_reg_wf %>%
  fit_resamples(train_folds)

#Collect cross-validation results
#log_reg_cv_results %>%
  #collect_metrics()

#Fit model to training data
#log_reg_fit <- log_reg_wf %>%
  #fit(train_data)

#Assess model on test data
#log_reg_wf %>%
  #augment(test) %>%
  #my_metrics(response, .pred)

#Use model to predict for new data
#log_reg_fit %>%
  #augment(new_data)
```

# Model Construction and Interpretation
In the following code, we will be using two training models: nearest neighbor and decision tree.The steps are to make a workflow, evaluate, and print the result.

```{r}
#knn workflow
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

#decision tree workflow
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

#evaluate workflows
knn_cv_results <- knn_wf %>%
  fit_resamples(train_folds)

dt_cv_results <- dt_wf %>%
  fit_resamples(train_folds)
```

```{r}
#knn result
knn_cv_results %>%
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

```{r}
#dt result
dt_cv_results %>%
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


Result Summary: the decision tree model has higher accuracy values

Next, we will be using hyperparameter tuning. Hyperparameter tuning allows data scientists to tweak model performance for optimal results. This process is an essential part of machine learning, and choosing appropriate hyperparameter values is crucial for success.

We will start off by setting the grid parameter of tune_grid() to a number which will create a “space filling” grid containing that number of hyperparameter combinations. These are essentially tibbles (data frames) of options for each hyperparameter.

```{r}

##grid
grid_neighbors <- tibble(
  "neighbors" = c(1, 3, 5, 7, 11, 15, 21, 41)
  )

grid_depth <- tibble(
  "tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20)
  )
```

The next step is to re-create the model workflows, setting the relevant hyperparameters to tune(). Then we’ll pipe these workflows into tune_grid() and read the results.

```{r}
##knn workflow
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

##dt workflow
dt_spec <- decision_tree(tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(priceRange ~ numOfBedrooms + numOfBathrooms + lotSizeSqFt, data = train_data) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

##hyperparameter tune
knn_tune_results <- knn_wf %>%
  tune_grid(
    grid = grid_neighbors,
    resamples = train_folds
  )

dt_tune_results <- dt_wf %>%
  tune_grid(
    grid = grid_depth,
    resamples = train_folds
  )

```

Here is the nearest neighbor result:
```{r}
##knn result 
knn_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(neighbors, mean, std_err) %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Here is the decission tree result:
```{r}
##dt results
dt_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(tree_depth, mean, std_err) %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


Result: The optimized nearest neighbor classifier has higher accuracy than the decision tree classifier. The accuracy for this model is about 38% which is slightly higher than the initial nearest neighbor result of 32%. It also beats decision tree model which has 37% accuracy. This proves that hyperparameter tuning can improve model and increase accuracy.