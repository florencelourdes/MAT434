---
title: "Hospital Length of Stay Models"
author: "Florence, Scientist"
format: html
---

```{r setup}
#install.packages("kknn")
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart.plot)
library(rpart)
library(tidyverse)
library(tidymodels)

hospital_data <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/master/data/classification/hospital_stays_smaller.csv")

#Create clean versions of column names by removing spaces and converting to lowercase
names(hospital_data) <- janitor::make_clean_names(names(hospital_data))
```

## Statement of Purpose

In the modern healthcare landscape, optimizing hospital operations and ensuring the efficient allocation of resources are paramount. One critical aspect of this process is predicting the length of a patient's stay in the hospital. This project aims to develop a robust prediction model for hospital length of stay, using data science and machine learning techniques, with a primary focus on utilizing the R programming language.

The problem we aim to address is the unpredictability of hospital length of stay, which can lead to challenges in resource allocation and patient management. By accurately predicting the length of stay, we can help hospitals optimize their resources, reduce costs, and enhance the overall patient experience.

## Introduction

Hospitals worldwide grapple with the challenge of managing their resources efficiently while ensuring high-quality patient care. The unpredictability of how long a patient will stay in the hospital adds a layer of complexity to this task. Hospitals need a solution that enables them to predict the length of a patient's stay accurately to allocate resources effectively and improve overall patient satisfaction.

## Exploratory Data Analysis

(Partially Omitted for now -- try on your own for practice...)

Major steps...

+ Explore data set (No summary statistics here...try to avoid data leakage)

  + Print out *head*
  + Are there missing values?
  + Are any classes in your response variable rare?

The first few rows of data appear below. 

```{r}
hospital_data %>%
  head()
```

Let's check the columns to see if there are missing values anywhere.

```{r}
hospital_data %>%
  summarize_all(~ sum(is.na(.))) #the dot (.) represents all columns here
```

Notice that there are missing values in the `bed_grade` and `city_code_patient` columns. There are very few missing values relative to the overall size of the data set. Now let's look at the distribution of the `stay` variable to see if there are any very rare classes.

```{r}
hospital_data %>%
  count(stay) %>%
  mutate(prop = 100*(n / sum(n)))
```

# Data Visualization
```{r}
# Create a bar plot for Age
ggplot(hospital_data, aes(x = age)) +
  geom_bar() +
  labs(title = "Distribution of Age",
       x = "Age",
       y = "Count")

# Create a bar plot for Age vs. Length Of Stay
ggplot(hospital_data, aes(x = age, fill = stay)) +
  geom_bar() +
  labs(title = "Age vs. Length of Stay",
       x = "Age",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Department vs. Length Of Stay
ggplot(hospital_data, aes(x = department, fill = stay)) +
  geom_bar() +
  labs(title = "Department vs. Length of Stay",
       x = "Department",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Severity vs. Length Of Stay
ggplot(hospital_data, aes(x = severity_of_illness, fill = stay)) +
  geom_bar() +
  labs(title = "Severity of Illness vs. Length of Stay",
       x = "Severity",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Type of Admission vs. Length Of Stay
ggplot(hospital_data, aes(x = type_of_admission, fill = stay)) +
  geom_bar() +
  labs(title = "Type of Admission vs. Length of Stay",
       x = "Admission",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title
```

There are certainly some categories here which are more rare than others. It looks like 11 - 40 day stays are most common. This encompasses three categories (11-20, 21-30, and 31-40 days). There are 6 categories in total here. Random guessing of classifications would lead us to an expected accuracy of less than 17%, but a naive classifier that just predicted 21 - 30 days for everyone would have nearly 30% accuracy. We'll see if we can build a classifier that does better than this.

+ **Do Today!** Split into training and test sets (be sure to set a seed)

We'll split the data into training and test sets below. Since we have such a large data set to begin with (100,000 observations), we can afford to put more than 75% of these observations into training.

```{r}
set.seed(123)
data_splits <- initial_split(hospital_data, prop = 0.9)
train <- training(data_splits)
test <- testing(data_splits)
```

+ Conduct EDA on *training* data only
  
  + Understand the distributions of your response variable and available predictors
  + Uncover associations between your response and the available predictors
  + Potentially probe for relationships between predictors -- this is particularly important if you want to interpret your model -- associated predictors cause problems for model interpretation when included in the same model
  + Summarize your findings and how they'll impact your model construction phase

```{r} 
# Create a bar plot for Age
ggplot(hospital_data, aes(x = age)) +
  geom_bar() +
  labs(title = "Distribution of Age",
       x = "Age",
       y = "Count")

# Create a bar plot for Age vs. Length Of Stay
ggplot(train, aes(x = age, fill = stay)) +
  geom_bar() +
  labs(title = "Age vs. Length of Stay",
       x = "Age",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Department vs. Length Of Stay
ggplot(train, aes(x = department, fill = stay)) +
  geom_bar() +
  labs(title = "Department vs. Length of Stay",
       x = "Department",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Severity vs. Length Of Stay
ggplot(train, aes(x = severity_of_illness, fill = stay)) +
  geom_bar() +
  labs(title = "Severity of Illness vs. Length of Stay",
       x = "Severity",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title

# Create a bar plot for Type of Admission vs. Length Of Stay
ggplot(train, aes(x = type_of_admission, fill = stay)) +
  geom_bar() +
  labs(title = "Type of Admission vs. Length of Stay",
       x = "Admission",
       y = "Count",
       fill = "Length of Stay") +
  theme(legend.title = element_blank())  # Remove legend title
```
The summary of the EDA on training data set is that middle aged people (31-40 and 41-50) have the longest length of stay. It also shows that gynecology department has the most patient and the longest stay. The most common severity for the longest stay is moderate, The trauma admission has the longest stay.

  
## Model Construction

Goal(s) for October 13, 2023

  + Build and assess an "off the shelf" *nearest neighbors classifier*
  + Tune a *nearest neighbors classifier* to find the optimal number of voting neighbors -- tune other hyperparameters if you like as well
  + Add your model's predictions as columns to your training set (being able to add predictions to a data frame will be useful for CA3)
  

### Nearest Neighbors Classifier

**Note:** Nearest neighbor algorithms scale poorly with additional training observations. This is because distances between any new observations and all training observations must be computed in order for a prediction to be made. Nearest neighbor algorithms can make find classifiers (or regressors), but are not typically viable with large datasets. We can, however, utilize parallel processing with this algorithm.

As mentioned, nearest neighbor classifiers are slow to fit and more training data means slower "fitting" times. To avoid exceedingly lengthy fit times, we'll take a small sample from our training data to work with -- just 10,000 observations -- and then create our cross-validation folds using that small sample.

```{r}
set.seed(456)
small_train <- train %>%
  slice_sample(n = 1e4)

set.seed(789)
small_folds <- vfold_cv(small_train, v = 10)
```

Now we're ready to build our nearest neighbor classifier. We'll start with an "off the shelf" model that accepts the default parameters. Additionally, we'll only use the `admission_deposit`, `visitors_with_patient`, and `available_extra_rooms_in_hospital` variables as predictors here since those are numerical predictors. The remaining features are probably best interpreted as categorical since there are very few observed levels of those variables.

```{r}
knn_clf_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(stay ~ admission_deposit + visitors_with_patient + available_extra_rooms_in_hospital, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

knn_wf <- workflow() %>%
  add_model(knn_clf_spec) %>%
  add_recipe(knn_rec)

knn_results <- knn_wf %>%
  fit_resamples(small_folds)
knn_results %>%
  collect_metrics()
```

Let's see the types of predictions this model makes. Let's `fit()` this nearest neighbors model and then use it to add a column of predictions to our small training dataset.

```{r}
knn_fit <- knn_wf %>%
  fit(small_train)

knn_fit %>%
  augment(small_train) %>%
  count(stay, .pred_class) %>%
  pivot_wider(names_from = .pred_class, values_from = n)
```

We aren't simply making the naive prediction, and it looks like the nearest neighbor classifier is making the expected mistakes. That is, when the classifier makes an erroneous prediction, it is most often to a neighboring class. Discussing neighboring classes here is meaningful since the `stay` variable is an *ordered* categorical variable.

## Summary

We've built and fit our nearest neighbor classifier and used it to make predictions on the small version of our training set. I'll leave it to you to use a grid search to find the optimal number of neighbors here. You'll want to use the small training set when you do this too -- otherwise your grid search and cross-validation will take a very, very long time to run.

We'll continue here from next time, where we'll build a decision tree classifier. At that class meeting, we'll try tuning hyperparameters for that class of model as well as our nearest neighbor classifier.

# Decision Tree Classifier
```{r}

# Create the decision tree model

## workflow
dt_spec <- decision_tree() %>%
  set_mode("classification") %>%
  set_engine("rpart")

dt_rec <- recipe(stay ~ age + department + severity_of_illness + type_of_admission, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

## cross validation
dt_cv_results <- dt_wf %>%
  fit_resamples(train)

## fit
trained_model <- fit(dt_wf, data = train)

## prediction
predictions <- predict(trained_model, new_data = test)

## hyperparameter grids
grid_depth <- tibble(
  "tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20)
  )
dt_tune_results <- dt_wf %>%
  tune_grid(grid = grid_depth, resamples = train)

## result
dt_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(tree_depth, mean, std_err) %>%
  arrange(-mean)
```



